{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_117sy0CGEU"
   },
   "source": [
    "# JAX 作为加速版的 NumPy\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/jax/blob/main/docs/jax-101/01-jax-basics.ipynb)\n",
    "\n",
    "*Authors: Rosalia Schneider & Vladimir Mikulik*\n",
    "\n",
    "本小节，你将学习到JAX的基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXjHL4L6ku3-"
   },
   "source": [
    "## JAX numpy\n",
    "\n",
    "首先，你可以先把JAX理解为*支持自动微分的NumPy，并且可以运行在GPU和TPU上面*。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZqUzvqF1B1TO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "x = jnp.arange(10)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPBmlAxXlBAy"
   },
   "source": [
    "是不是和NumPy一样，JAX最吸引人的地方就是你不需要重新学习一套API，只需要将`np`替换为`jnp`，大多数NumPy程序就可以无缝切换到JAX，当然，二者还是有区别的，第一个区别就是数组类型（dtype），JAX数组类型是 `DeviceArray`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3fLtgPUAn7mi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yx8VofzzoHFH"
   },
   "source": [
    "JAX的一大优势是同一个代码，即可以运行在CPU，也可以运行在GPU或TPU。\n",
    "\n",
    "来看一个点积的例子，使用 `%timeit` 来记录运行时间。\n",
    "\n",
    "当JAX代码执行时，相应的算子/计算操作会尽量被分发到加速卡上面异步执行，执行结果也没必要立即返回到host，此时`%timeit`记录到仅仅是程序分发的时长，所以我们使用`block_until_ready()`来记录程序真正的执行时长可以参考 [Asynchronous dispatch](https://jax.readthedocs.io/en/latest/async_dispatch.html#asynchronous-dispatch)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mRvjVxoqo-Bi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.37 ms ± 149 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "long_vector = jnp.arange(int(1e7))\n",
    "\n",
    "%timeit jnp.dot(long_vector, long_vector).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkCpI-v0uQQO"
   },
   "source": [
    "## JAX : `grad`\n",
    "\n",
    "JAX的一大特点就是可以对Python函数进行多种转换（transformation）。我们来看第一个要学习的转换 `jax.grad`，它能对一个输出是标量的函数，计算其导函数。\n",
    "\n",
    "我们来看一个例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LuaGUVRUvbzQ"
   },
   "outputs": [],
   "source": [
    "def sum_of_squares(x):\n",
    "  return jnp.sum(x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAqloI1Wvtp2"
   },
   "source": [
    "使用 `jax.grad` 来得到 `sum_of_squares` 的导函数（一个新的函数）。注意新的导函数只能计算第一个传参的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dKeorwJfvpeI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n",
      "[2. 4. 6. 8.]\n"
     ]
    }
   ],
   "source": [
    "sum_of_squares_dx = jax.grad(sum_of_squares)\n",
    "\n",
    "x = jnp.asarray([1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "print(sum_of_squares(x))\n",
    "\n",
    "print(sum_of_squares_dx(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares_multi(x, y):\n",
    "  return jnp.sum(x**2 + y**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.0\n",
      "[2. 4. 6. 8.]\n"
     ]
    }
   ],
   "source": [
    "sum_of_squares_multi_dx_dy = jax.grad(sum_of_squares_multi)\n",
    "\n",
    "x = jnp.asarray([1.0, 2.0, 3.0, 4.0])\n",
    "y = jnp.asarray([2.0, 3.0, 4.0, 6.0])\n",
    "\n",
    "print(sum_of_squares_multi(x, y))\n",
    "\n",
    "print(sum_of_squares_multi_dx_dy(x, y))  # 只计算了第一个传承x的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfBt5CYbyKUX"
   },
   "source": [
    "你可以把 `jax.grad` 理解为微分中的梯度符号， $\\nabla$ 。 给定函数 $f(x)$, $\\nabla f$ 表示导函数\n",
    "\n",
    "$$\n",
    "(\\nabla f)(x)_i = \\frac{\\partial f}{\\partial x_i}(x).\n",
    "$$\n",
    "\n",
    "所以，`jax.grad(f)` 就是导函数，从而 `jax.grad(f)(x)` 就是对 `x`求梯度。\n",
    "\n",
    "（和 $\\nabla$一眼， `jax.grad` 只作用于返回标量的函数。）\n",
    "\n",
    "\n",
    "这使得JAX API和其他的自动微分框架，比如TF和PyTorch大不相同，后两者是根据loss tensor来计算梯度，也就是调用 `loss.backward()` 。而JAX直接作用于**函数**，和数学课上学的那样。一旦你理解了，这么做就很自然了，你写的损失函数本质上就是一个关于参数和数据的函数，在数学课堂上，我们就是通过计算导函数来计算参数梯度的。\n",
    "\n",
    "默认情况下， `jax.grad` 只对第一个函数传参求导："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "f3NfaVu4yrQE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.20000005 -0.19999981 -0.19999981 -0.19999981]\n"
     ]
    }
   ],
   "source": [
    "def sum_squared_error(x, y):\n",
    "  return jnp.sum((x-y)**2)\n",
    "\n",
    "sum_squared_error_dx = jax.grad(sum_squared_error)\n",
    "\n",
    "y = jnp.asarray([1.1, 2.1, 3.1, 4.1])\n",
    "\n",
    "print(sum_squared_error_dx(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tOztA5zpLWN"
   },
   "source": [
    "如果我们要计算对其他传参的梯度，甚至是多个变量，可以使用 `argnums`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FQSczVQkqIPY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([-0.20000005, -0.19999981, -0.19999981, -0.19999981], dtype=float32),\n",
       " DeviceArray([0.20000005, 0.19999981, 0.19999981, 0.19999981], dtype=float32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.grad(sum_squared_error, argnums=(0, 1))(x, y)  # 对x和y都计算梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQAMTnZSqo-t"
   },
   "source": [
    "`jax.grad`很好用，可是神经网络通常包含了Million级别的参数，难道我们要一一列出来，写一个巨长的传参列表，计算梯度？当然不是了，JAX提供饿了一种数据结构叫做 'pytrees'，可以将参数打包，一把计算梯度，更多信息可以看 [later guide](https://colab.research.google.com/github/google/jax/blob/main/docs/jax-101/05.1-pytrees.ipynb)。但是大致是这样的：\n",
    "\n",
    "```\n",
    "def loss_fn(params, data):\n",
    "  ...\n",
    "\n",
    "grads = jax.grad(loss_fn)(params, data_batch)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBowiovisT97"
   },
   "source": [
    "其中 `params` 可以是一个数组构成的dict，而返回的 `grads` 是也是数组构成的dict。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNjf9jUEsZZ8"
   },
   "source": [
    "## Value and Grad\n",
    "\n",
    "有的时候，对于一个函数，我们既想得到函数值也想得到导函数值（对变量的梯度），比如我们会记录下训练集的loss，JAX提供了一个转换，很好用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dWg4_-h3sYwl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(0.03999995, dtype=float32),\n",
       " DeviceArray([-0.20000005, -0.19999981, -0.19999981, -0.19999981], dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.value_and_grad(sum_squared_error)(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVT2EWHJsvvv"
   },
   "source": [
    "返回的是一个tuple，(value, grad)。\n",
    "\n",
    "```\n",
    "jax.value_and_grad(f)(*xs) == (f(*xs), jax.grad(f)(*xs)) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmHTVpAks3OX"
   },
   "source": [
    "## Auxiliary data\n",
    "\n",
    "除了记录最终的函数值，我们也可能想记录某些中间结果，是否可以让函数返回？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ffGCEzT4st41",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Gradient only defined for scalar-output functions. Output was (DeviceArray(0.03999995, dtype=float32), DeviceArray([-0.10000002, -0.0999999 , -0.0999999 , -0.0999999 ], dtype=float32)).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/core.py:1100\u001b[0m, in \u001b[0;36mget_aval\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1100\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_aval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/core.py:1092\u001b[0m, in \u001b[0;36mconcrete_aval\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1091\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete_aval(x\u001b[38;5;241m.\u001b[39m__jax_array__())\n\u001b[0;32m-> 1092\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid JAX \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1093\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Value (DeviceArray(0.03999995, dtype=float32), DeviceArray([-0.10000002, -0.0999999 , -0.0999999 , -0.0999999 ], dtype=float32)) with type <class 'tuple'> is not a valid JAX type",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msquared_error_with_aux\u001b[39m(x, y):\n\u001b[1;32m      2\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m sum_squared_error(x, y), x\u001b[38;5;241m-\u001b[39my\n\u001b[0;32m----> 4\u001b[0m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43msquared_error_with_aux\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 4 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/api.py:1015\u001b[0m, in \u001b[0;36m_check_scalar\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1013\u001b[0m   aval \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mget_aval(x)\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1015\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwas \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1017\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(aval, ShapedArray):\n",
      "\u001b[0;31mTypeError\u001b[0m: Gradient only defined for scalar-output functions. Output was (DeviceArray(0.03999995, dtype=float32), DeviceArray([-0.10000002, -0.0999999 , -0.0999999 , -0.0999999 ], dtype=float32))."
     ]
    }
   ],
   "source": [
    "def squared_error_with_aux(x, y):\n",
    "  return sum_squared_error(x, y), x-y\n",
    "\n",
    "jax.grad(squared_error_with_aux)(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUubno3nth4i"
   },
   "source": [
    "显然不行，前面我们提过 `jax.grad` 只能作用于返回标量的函数，现在返回的是tuple，可以用`has_aux`参数:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "uzUFihyatgiF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([-0.20000005, -0.19999981, -0.19999981, -0.19999981], dtype=float32),\n",
       " DeviceArray([-0.10000002, -0.0999999 , -0.0999999 , -0.0999999 ], dtype=float32))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.grad(squared_error_with_aux, has_aux=True)(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5s3UiFauwDk"
   },
   "source": [
    "`has_aux` 表明函数返回 `(out, aux)`，`aux`是原始函数的第二个返回结果，而`out`是梯度值，它让 `jax.grad` 忽略 `aux`，专心计算梯度。\n",
    "\n",
    "有点`value_and_grad`的味道。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fk4FUXe7vsW4"
   },
   "source": [
    "## 和NumPy的差异\n",
    "\n",
    "`jax.numpy` API 虽然和NumPy很像，但还是有区别的，最重要的区别是，JAX是函数式的编程范畴。\n",
    "\n",
    "本文不会介绍什么是函数式编程，如果你对此不了解也没关系，只需要记住一点：不要写有副作用（side-effect）的函数就行了。\n",
    "\n",
    "函数副作用指的是任何没有体现在函数返回值上面的影响，比如修改了全局变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "o_YBuLQC1wPJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([123,   2,   3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([1, 2, 3])\n",
    "\n",
    "def in_place_modify(x):\n",
    "  x[0] = 123\n",
    "  return None\n",
    "\n",
    "in_place_modify(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "u6grTYIVcZ3f",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<class 'jaxlib.xla_extension.DeviceArray'>' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43min_place_modify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36min_place_modify\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21min_place_modify\u001b[39m(x):\n\u001b[0;32m----> 6\u001b[0m   x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m123\u001b[39m\n\u001b[1;32m      7\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:4568\u001b[0m, in \u001b[0;36m_unimplemented_setitem\u001b[0;34m(self, i, x)\u001b[0m\n\u001b[1;32m   4563\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_unimplemented_setitem\u001b[39m(\u001b[38;5;28mself\u001b[39m, i, x):\n\u001b[1;32m   4564\u001b[0m   msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object does not support item assignment. JAX arrays are \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4565\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimmutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4566\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor another .at[] method: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4567\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 4568\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)))\n",
      "\u001b[0;31mTypeError\u001b[0m: '<class 'jaxlib.xla_extension.DeviceArray'>' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html"
     ]
    }
   ],
   "source": [
    "in_place_modify(jnp.array(x))  # Raises error when we cast input to jnp.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Rmklk6BB2xF0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([123,   2,   3], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jax_in_place_modify(x):\n",
    "  return x.at[0].set(123)\n",
    "\n",
    "y = jnp.array([1, 2, 3])\n",
    "jax_in_place_modify(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91tn_25vdrNf"
   },
   "source": [
    "现在反悔了一个新数组，原数组没有改动，所以函数没有副作用:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "KQGXig4Hde6T"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([1.1, 2.1, 3.1, 4.1], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5TibzPO25qa"
   },
   "source": [
    "没有副作用的函数也被称为纯函数。如果你要用JAX转换，就要写纯函数。有的时候写不出来？可以寻求JAX中丰富的原语来帮助。\n",
    "\n",
    "Isn't the pure version less efficient? Strictly, yes; we are creating a new array. However, as we will explain in the next guide, JAX computations are often compiled before being run using another program transformation, `jax.jit`. If we don't use the old array after modifying it 'in place' using indexed update operators, the compiler can recognise that it can in fact compile to an in-place modify, resulting in efficient code in the end.\n",
    "\n",
    "Of course, it's possible to mix side-effectful Python code and functionally pure JAX code, and we will touch on this more later. As you get more familiar with JAX, you will learn how and when this can work. As a rule of thumb, however, any functions intended to be transformed by JAX should avoid side-effects, and the JAX primitives themselves will try to help you do that.\n",
    "\n",
    "We will explain other places where the JAX idiosyncracies become relevant as they come up. There is even a section that focuses entirely on getting used to the functional programming style of handling state: [Part 7: Problem of State](https://colab.research.google.com/github/google/jax/blob/main/docs/jax-101/07-state.ipynb). However, if you're impatient, you can find a [summary of JAX's sharp edges](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html) in the JAX docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFn_VBFFlGCz"
   },
   "source": [
    "## Your first JAX training loop\n",
    "\n",
    "看一个线性回归的例子。\n",
    "\n",
    "Our data is sampled according to $y = w_{true} x + b_{true} + \\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "WGgyEWFqrPq1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xs = np.random.normal(size=(100,))\n",
    "noise = np.random.normal(scale=0.1, size=(100,))\n",
    "ys = xs * 3 - 1 + noise\n",
    "\n",
    "plt.scatter(xs, ys);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTh22mo4rR1x"
   },
   "source": [
    "Therefore, our model is $\\hat y(x; \\theta) = wx + b$.\n",
    "\n",
    "We will use a single array, `theta = [w, b]` to house both parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "TnVrRTMamyzb"
   },
   "outputs": [],
   "source": [
    "def model(theta, x):\n",
    "  \"\"\"Computes wx + b on a batch of input x.\"\"\"\n",
    "  w, b = theta\n",
    "  return w * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCrLmmKrn9_h"
   },
   "source": [
    "The loss function is $J(x, y; \\theta) = (\\hat y - y)^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "07eMcDLMn9Ww"
   },
   "outputs": [],
   "source": [
    "def loss_fn(theta, x, y):\n",
    "  prediction = model(theta, x)\n",
    "  return jnp.mean((prediction-y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejMt4dulnoYX"
   },
   "source": [
    "How do we optimize a loss function? Using gradient descent. At each update step, we will find the gradient of the loss w.r.t. the parameters, and take a small step in the direction of steepest descent:\n",
    "\n",
    "$\\theta_{new} = \\theta - 0.1 (\\nabla_\\theta J) (x, y; \\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "2I6T5Wphpaaa"
   },
   "outputs": [],
   "source": [
    "def update(theta, x, y, lr=0.1):\n",
    "  return theta - lr * jax.grad(loss_fn)(theta, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAUL1gT_opVn"
   },
   "source": [
    "In JAX, it's common to define an `update()` function that is called every step, taking the current parameters as input and returning the new parameters. This is a natural consequence of JAX's functional nature, and is explained in more detail in [The Problem of State](https://colab.research.google.com/github/google/jax/blob/main/docs/jax-101/07-state.ipynb).\n",
    "\n",
    "This function can then be JIT-compiled in its entirety for maximum efficiency. The next guide will explain exactly how `jax.jit` works, but if you want to, you can try adding `@jax.jit` before the `update()` definition, and see how the training loop below runs much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "WLZxY7nIpuVW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: 3.00, b: -1.00\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAZRklEQVR4nO3de5BcZZnH8d8znQ7pAcIECWAGxiBiUDYm0RaiKMpFAgmEGBVkxfWeKm9L1A0kAgoLbLJmxeBqlZtV13JhJUqyIxRiSLwulAEmzJBwC6IESAclaCZgMknm8uwfc2Gmp3v6drpPn+7vp8oi5/Tpc94h8sub97zv+5i7CwAQXQ1hNwAAUBqCHAAijiAHgIgjyAEg4ghyAIi4cWE89KijjvKpU6eG8WgAiKzNmze/6O6T08+HEuRTp05VW1tbGI8GgMgys2cynWdoBQAijiAHgIgjyAEg4ghyAIg4ghwAIo4gB4CIC2X6IQDUm9b2lFau36adnV2a0pTQkjnTtGBWcyD3JsgBoMxa21O6cu0WHejpkySlOru0bN1WSQokzBlaAYAycnctXfdKiA/q6u7VyvXbAnlGIEFuZk1mdruZPWFmj5vZ24K4LwBE2dYde3TCsp9pf3dfxs93dnYF8pyghlZulvRzd3+/mY2X1BjQfQEgcvr6XB/4j99p8zO7JUkNJvVlKMY2pSkRyPNKDnIzO0LSGZI+KknuflDSwVLvCwBRdN9TL+pD371/6PgHH3urOvd1a9m6rerq7h06n4jHtGTOtECeGUSP/ARJuyT9l5nNkLRZ0uXuvjeAewNAJHT39undK3+t1MBwySlTJuqOz71DsQYbuqZcs1as1OLLZpaUtEnS6e5+v5ndLOkld78m7bpFkhZJUktLy1ueeSbjJl4AEDl3bXlen/2fh4aO133m7Xpzy6TAn2Nmm909mX4+iB75Dkk73H3w7xK3S1qafpG7r5a0WpKSyWRpf3oAQBXYd7BHM667R929/ZF21slH63sfScrMcnwzWCUHubv/ycyeM7Np7r5N0tmSHiu9aQBQvf570zO6pvWRoeMNXzhDJx1zeChtCWrWyucl3TowY+WPkj4W0H0BoKrs3ntQs67fMHR86aktWr5weogtCijI3b1D0qhxGwCoJas2PqlVG38/dHzf0rPUHNAUwlKwRB8ActjZ2aW3r/jl0PE/nn2Svvie14fYopEIcgAYw9WtW3XLpmeHjh+65j068tDxIbZoNIIcADJ46oWXdc5Nvx06vm7+KfrI26eG16AxEOQAMKC1PaWv/fwJ7dyzf+hcg0lbr52jQw+p3ris3pYBQAW1tqe05CcPq3vYpijxmGnl+2dUdYhLbGMLAOrp7dPiNR0jQlySuns9sK1my6m6/5gBgIAMr9DT1BiXu7Snq1uHHjJOfzvQk/V7QW01W04EOYCa19qeGrH74O593UOfjRXiUnBbzZYTQysAat7K9dtGbCGbyaTGuBLx2IhzQW41W04EOYCal8/wSOe+bi1fOF3NTQmZpOamhJYvnB7YVrPlxNAKgJo3pSkxtE/4WNcsmNUcieBOR48cQE370QPP5gzxqAyhZEOPHEDNGD4zJZ9euCTFzCIzhJINPXIANWFwZkqqs0sujQrxVZfMzPgy8+sXz4h0iEv0yAHUiGwzUyY1xtX+lXNHXFeOuplhIsgB1IRswyidw+aMR/VlZi4MrQCItN4+19Sld2X9PAoLekoVWI/czGKS2iSl3P2CoO4LANmMFeBS/6ZXUZ6Nkq8ge+SXS3o8wPsBQEa79x7MGeKSdOj4cTU5lJIukB65mR0naZ6kGyV9MYh7Aqhv6VMJB19MZgpwk+Sjb6E9Xd0ZztaeoHrkqyRdIakvoPsBqGOZphJeuXbLqBB/4vrztH3FvKzj4PUwPi4FEORmdoGkF9x9c47rFplZm5m17dq1q9THAqhhmaYSHugZ2U/cvmKeJgzMC18yZ1pkN7wKQhBDK6dLmm9mcyVNkDTRzG5x98uGX+TuqyWtlqRkMpnpb0EAIGnsTa6eXj5XZjbi3OA4eC3OEc9HyUHu7sskLZMkM3u3pH9KD3EAKES25fXNTYlRIT6oVueI54N55ACqytK1WzKGeD0NlRQq0JWd7v5rSb8O8p4A6kf6y8wJ8QYd6O6ru6GSQrFEH0DoTr1xo154+cCIc9tXzAupNdFDkAMIVXov/JoL3qhPvOOEkFoTTQQ5gFBkWthDL7w4BDmAiuru7dNJV9094tyaRbN12mtfFVKLoo8gB1Ax9MLLgyAHUHYvvLxfp974ixHnNi07W8ceMSGkFtUWghxAIAY3uUp1dilmpl53NWdZ2EMvPFgEOYCSDW5yNbg/Sq/378KRHuJP3nC+xo9jHWLQCHIAJctWL3M4euHlwx+NAEo21iZXUv9+4SgfghxAyY5IxMf8vF72BQ8LQysACja8ek8+e1Kz2VV5EeQA8jJ8Vkq20mqZNCXibHZVZgQ5gJzSZ6XkG+KJeEzXzj+lfA2DJMbIAeQhn1kp6WJmWr5wOr3xCiDIAeSUa1ZKukQ8pq9fPIMQrxCCHMCYbnvg2ZxDKfEG06TGuEz95djoiVcWY+QARhn+YjObwReezVTvCV3JQW5mx0v6oaRj1P/7utrdby71vgDC0dqe0hW3b9HB3r6s18TMGDqpIkH0yHskfcndHzKzwyVtNrMN7v5YAPcGUAGFzgvvcyfEq0jJQe7uz0t6fuDXL5vZ45KaJRHkQBUaHtpTmhI68+TJWrs5VdCsFFZqVpdAx8jNbKqkWZLuz/DZIkmLJKmlpSXIxwLIU/p88FRnl27Z9GxB90jEY6zUrDKBzVoxs8MkrZW02N1fSv/c3Ve7e9Ldk5MnTw7qsQAKUMx8cOmVTa+YkVKdAumRm1lc/SF+q7uvC+KeAILT2p7SdXc+qt37ugv+LrNSql8Qs1ZM0vckPe7uN5XeJABBam1PacntD6u7N9+F9a9obkrovqVnlaFVCFIQQyunS/qwpLPMrGPgf3MDuC+AAKxcv62oEGcsPDqCmLVyr9g3Hqg6+SzqGe7Q8TE1NY4fms3CcEp0sLITqEFXt27VrZtyL60fFI+ZbnwvLzGjiiAHagAvM+sbQQ5EWGt7Skt+0qHu7Kvps5rUGFf7V84NvlGoOIIciKjW9pS+uKZDRWS4EvGYvnohBR9qBUEORNTK9duKCnGGUmoPQQ5EzNWtW/Wj+59Trxc2pfCy2S26YcH0MrUKYSLIgSqVvrnVkjnT1PbMXwveG0WSTj/xSEK8hhHkQBXKtLnV4jUdBd/HJH2InnjNI8iBKlTs5laDmhJxXTv/FMbB6wRBDlShQosdx8z0h+XsjFGvKL4MVJnW9pQarLBdLy497fgytQZRQI8cqCKDY+P5zkiJmenS045nDLzOEeRAiIbPTEnEG7SvgCWa21fMK2PLECUEORCS9JkphYR4MzUzMQxj5EBIip2Zwj7hSEeQAyEpZGZKUyIuEzUzkRlDK0BIpjQl8ir60JSIq+Or7FKI7IIqvnyepJslxSR9191XBHFfIMoyLbEf7El/6ccP5xXiDZKunc8uhRibeYEb74y6gVlM0pOS3iNph6QHJV3q7o9l+04ymfS2traSngtUs0Ir9JikRLxBXd19Q99JxBu0fOGbGEbBEDPb7O7J9PNB9MhPlfSUu/9x4EG3SbpIUtYgB2pZa3sq7xCfMK5BK95HWKM0QbzsbJb03LDjHQPnRjCzRWbWZmZtu3btCuCxQHVauX5b3j3x/T19Wrl+W1nbg9pXsVkr7r7a3ZPunpw8eXKlHgtUTGt7SrP++Z68q9YPKnRfFSBdEEMrKUnDN3o4buAcUDda21NacvvD6u4t/J3TFBb3oERBBPmDkk4ysxPUH+AflPT3AdwXqFrpM1J27z1QVIizuAdBKDnI3b3HzD4nab36px9+390fLbllQJXKVPShGNTORFACmUfu7j+T9LMg7gVUu2KW1pskF+GN8mBlJ1CgQl9OTmqM66sXUq0H5UOQAwXKd2m9SfrGJTMJcJQdm2YBBVowa0rOaxLxGCGOiqFHDowhfXZKvi822aEQlUSQA1m0tqe0eE3H0HF6iB87cYL+9NL+Ud9rbkoQ4qgoghxIM9gLH6v3vX3FvFHTECXmhSMcBDkwTCErNAd73dm2qgUqhSAHhrnuzkcLWqG5YFYzwY3QMWsFGGb3vu6wmwAUjB456lZre0rX3vGoOrsKC+/TTzyyTC0CikOQoy61tqe05CcPq7uvsI2uTj/xSN36qbeVqVVAcQhy1KVr73g07xCPN5hWfmAGY+GoWoyRo+60tqdyDqc0NyVkA/8kxFHt6JGjruzZ1z1ikU8mzU0J3bf0rAq1CCgdQY6aVczy+njMWNCDyCHIUZOKKf7QYNLK9zOMguhhjBw1pbU9pdNX/FKL13RkLf6w6pKZSsRjI84l4jHddDG7FSKaSuqRm9lKSRdKOijpD5I+5u6dQTQMKFSmvU8yYWk9ak2pQysbJC0bqNv5r5KWSbqy9GYBhcunBFvMTBJL61FbShpacfd73L1n4HCTpONKbxJQnHzGwXu98Er3QLULcoz845LuzvahmS0yszYza9u1a1eAjwWkqUvvyuu65qZEmVsCVF7OoRUz2yjp2AwfXeXuPx245ipJPZJuzXYfd18tabUkJZNJukUIRL4BLrFXOGpXziB393PG+tzMPirpAklnu/P3VpRHa3tK19356NDuhIl4g7q6+0Zdt+qSmUMvMZsa43KX9nR180ITNa3UWSvnSbpC0rvcfV8wTQJGylTsIT3EE/HYUJ1Mwhr1ptQx8m9JOlzSBjPrMLPvBNAmYMjVrVu1eE1HzmIPXd29Wrl+W4VaBVSXknrk7v66oBoCpLu6datu2fRs3tfvzLPCPVBrWKKPqlNswYcpzEhBnSLIUVWKLfjAjBTUM4IcVWXl+m15hXgi3qAJ8Zg69zEjBSDIUVXyWZ152ewW3bBgegVaA0QDQY6qsOiHbbrnsT/nvI4QB0YjyFFR6cUelsyZlrNij9Rf8IG9woHM2I8cFTO4zWyqs0uu/mGU9BDfvmKeVl0yU02J+NC5SY1xQhwYAz1ylE1673vvgZ4xt5ndvmKeJLaYBQpFjxxlkan3Pda88EQ8ptb2VOUaCNQQghxlkU+Rh+FYYg8UjyBHWRSzXJ4l9kBxCHKURbbl8g32Srm1fL8DYGwEOQLX2p7KuLBnsFL91y+ekbGKPUvsgeIwawVFS9/calJjXGeePFnrHto56tpJjXF99cJTRsxGoYo9EAyCHEXJtLnV7n3dGUNckhrHjxsR1EwxBILD0AqKku/mVoN4kQmUD0GOouSzudVwvMgEyieQIDezL5mZm9lRQdwP1a3QhTu8yATKq+QgN7PjJZ0rKf+aXIi0XJtcNcYb1NyUkElqbkoMFUUGUB5BvOz8hqQrJP00gHuhip1/8//p8edfynndvyx8E8ENVFBJQW5mF0lKufvDlmWRx7BrF0laJEktLS2lPBYhmLr0rryuu2x2CyEOVFjOIDezjZKOzfDRVZK+rP5hlZzcfbWk1ZKUTCYLK8iI0GQK8FWXzNSydVtH7KVikj5E0QcgFDmD3N3PyXTezKZLOkHSYG/8OEkPmdmp7v6nQFuJUKSH+BtePVF3X/7OoWMW9ADVoeihFXffKunowWMz2y4p6e4vBtAuhChTL3xwr/BBLOgBqgfzyDGkt88zhjh7hQPVLbAl+u4+Nah7oTIGK/jkWtwzuFc4PXCgOrHXSp0arOCTb/EHltgD1Ysgr3GZqtYvmNVccAUfltgD1Ysgr2Hpve5UZ5eWrduqp1/cW/BeKSyxB6oXLztrWKZed1d3r27+xe8Luk9TIs74OFDFCPIalmtce8K43L/9iXhM184/JagmASgDgryGjTWuvX3FPK1435vUPHDNYB3NSY1xNSXibHgFRAhj5DXs6MMPGTUWnojHtHxh/zJ6FvUAtYEgrxHps1MyvcxsZik9UJMI8hqQaXbKcOnL6wHUFsbIa0C2OeGHHTKOEAfqAEFeA7LNCd97oKfCLQEQBoI8wnp6+8Ys+MBqTKA+MEYeUbkq9lDwGKgf9Mgj5i9/OzAqxO+98kytumQmBY+BOkWPPELGKvhw3KRGghuoUwR5BDz8XKcu+vZ9I849cf15mhCPhdQiANWEIK9y+ZRdA1DfSg5yM/u8pM9K6pV0l7tfUXKroDUPPqsr124dce7p5XM1UOgaAIaUFORmdqakiyTNcPcDZnZ0ru9gbK3tKS1e0zHiXDxm+v2Nc0NqEYBqV2qP/NOSVrj7AUly9xdKb1L9+vgPHtQvnxj5r3D4JlcAkEmp0w9fL+mdZna/mf3GzN6a7UIzW2RmbWbWtmvXrhIfW3umLr1rVIhLrxQ+BoBscvbIzWyjpGMzfHTVwPePlDRb0lsl/djMXuvunn6xu6+WtFqSksnkqM/r1fxv3astO/aMeQ2FjwGMJWeQu/s52T4zs09LWjcQ3A+YWZ+koyTR5c5D+oyUSY1x7d7XPeo6ltoDGEupY+Stks6U9Csze72k8ZJeLLlVNS7blML07WglltoDyK3UIP++pO+b2SOSDkr6SKZhlXo2vODDq4+YoJ179o/4/M7PvUPTjztCkoZWZg4vEEEhCAC5WBi5m0wmva2treLPrbRMPezhWNgDoBBmttndk+nnWdkZsOE98AYz9Wb4g7JB0k2XzKx84wDUJII8QOk98EwhLkl9kpat61+1ybAJgFKxjW2AspVcy4T54QCCQo+8SOlV65fMmVbwfG/mhwMIAkFehExV669cu0WFvjZmfjiAIDC0UoRMQygHevoyXpuIx3TZ7BYl0vYOZ344gKAQ5EUYa0gkU8m1GxZM1/KF0ynFBqAsGFopwpSmhFIZwry5KaEFs5ozBnS28wBQKnrkBfrVthcyhjhDJQDCQo88T+6uE5b9bMS5YydO0J9f2s9SegChIsjzkF527R2vO0q3fPK0EFsEAK8gyMfQ2+c68csje+Fbrj1XEyfEQ2oRAIxGkGdx0z3b9M1fPjV0fNnsFt2wgJJrAKoPQZ5mf3evTr7m5yPOPXnD+Ro/jvfCAKoTQT7M5be166cdO4eOvzz3ZC0648QQWwQAuRHkkv6696DefP2GEeeeXj5XZhZSiwAgf3Uf5Bf++73amnql+PE3L52l+TOmhNgiAChMSUFuZjMlfUfSBEk9kj7j7g8E0bBye+Yve/Wulb8ecY6KPQCiqNQe+dckXefud5vZ3IHjd5fcqjJ7/dV36+CwTa5uWzRbs1/7qhBbBADFKzXIXdLEgV8fIWnnGNeGruO5Ti349n0jztELBxB1pQb5Yknrzezf1L9vy9uzXWhmiyQtkqSWlpYSH1u4qUvvGnG84Qtn6KRjDq94OwAgaDmD3Mw2Sjo2w0dXSTpb0hfcfa2ZXSzpe5LOyXQfd18tabUkJZPJQmsw5JSpYs+CWc3a+Nif9ckftg1d95pXNeo3S84M+vEAEBrzLAWC8/qy2R5JTe7u1j9Xb4+7T8z1vWQy6W1tbbkuy1t6xR5JmjCuQfvTij3c/+WzdczECYE9FwAqycw2u3sy/XypyxV3SnrXwK/PkvT7Eu9XlEwVe4aH+FknH63tK+YR4gBqUqlj5J+SdLOZjZO0XwNj4JU2VsWeR66bo8MOqfvp8gBqWEkJ5+73SnpLQG0p2lgVewhxALUu8jtBHezp094DPaPOU7EHQL2ITHc106yUWIPp8z9qH7pm8mGH6MW/HaBiD4C6EokgT5+Vkurs0uI1HUOfn/OGo/Wf/5BkkysAdSkSQZ5pVsqgjV88Q687moU9AOpXJMbIs81KMYkQB1D3IhHkU5oSBZ0HgHoSiSBfMmeaEvHYiHPMSgGAfpEYIx+cfZJpLxUAqHeRCHKpP8wJbgAYLRJDKwCA7AhyAIg4ghwAIo4gB4CII8gBIOIIcgCIuJJKvRX9ULNdkp6p+IPL5yhJL4bdiAri56199fYzR+XnfY27T04/GUqQ1xoza8tUR69W8fPWvnr7maP+8zK0AgARR5ADQMQR5MFYHXYDKoyft/bV288c6Z+XMXIAiDh65AAQcQQ5AEQcQR4AM1tpZk+Y2RYz+18zawq7TeVmZh8ws0fNrM/MIjttKxczO8/MtpnZU2a2NOz2lJOZfd/MXjCzR8JuSyWY2fFm9isze2zg/8uXh92mYhHkwdgg6e/c/U2SnpS0LOT2VMIjkhZK+m3YDSkXM4tJ+rak8yW9UdKlZvbGcFtVVj+QdF7YjaigHklfcvc3Spot6bNR/f0lyAPg7ve4e8/A4SZJx4XZnkpw98fdfVvY7SizUyU95e5/dPeDkm6TdFHIbSobd/+tpL+G3Y5Kcffn3f2hgV+/LOlxSZGsXkOQB+/jku4OuxEIRLOk54Yd71BE/0PH2MxsqqRZku4PtyXFiUypt7CZ2UZJx2b46Cp3/+nANVep/69rt1aybeWSz88MRJ2ZHSZpraTF7v5S2O0pBkGeJ3c/Z6zPzeyjki6QdLbXyOT8XD9zHUhJOn7Y8XED51AjzCyu/hC/1d3Xhd2eYjG0EgAzO0/SFZLmu/u+sNuDwDwo6SQzO8HMxkv6oKQ7Qm4TAmJmJul7kh5395vCbk8pCPJgfEvS4ZI2mFmHmX0n7AaVm5m918x2SHqbpLvMbH3YbQrawAvsz0lar/4XYT9290fDbVX5mNmPJP1O0jQz22Fmnwi7TWV2uqQPSzpr4L/bDjObG3ajisESfQCIOHrkABBxBDkARBxBDgARR5ADQMQR5AAQcQQ5AEQcQQ4AEff/wA5ga+Fcz+UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "theta = jnp.array([1., 1.])\n",
    "\n",
    "for _ in range(1000):\n",
    "  theta = update(theta, xs, ys)\n",
    "\n",
    "plt.scatter(xs, ys)\n",
    "plt.plot(xs, model(theta, xs))\n",
    "\n",
    "w, b = theta\n",
    "print(f\"w: {w:<.2f}, b: {b:<.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-q17kJ_rjLc"
   },
   "source": [
    "As you will see going through these guides, this basic recipe underlies almost all training loops you'll see implemented in JAX. The main difference between this example and real training loops is the simplicity of our model: that allows us to use a single array to house all our parameters. We cover managing more parameters in the later [pytree guide](https://colab.research.google.com/github/google/jax/blob/main/docs/jax-101/05.1-pytrees.ipynb). Feel free to skip forward to that guide now to see how to manually define and train a simple MLP in JAX."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Jax Basics.ipynb",
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
