{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtWX4x9DCF5_"
   },
   "source": [
    "# JAX 快速入门\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/quickstart.ipynb)\n",
    "\n",
    "**JAX是可以在CPU、GPU和TPU上运行的NumPy，并且支持自动微分（automatic differentiation），可用于机器学习领域。**\n",
    "\n",
    "作为[Autograd](https://github.com/hips/autograd)项目的升级版，JAX可以对原始Python以及Numpy代码进行自动微分计算。 JAX的的自动微分功能非常强大：\n",
    "1. 支持大量Python语法特性，包括循环（loop）、条件判断（if-else）、递归（recursion）和闭包（closure）；\n",
    "2. 支持高阶求导；\n",
    "3. 支持reverse-mode和forward-mode两种微分方式，并且二者可以任意组合。\n",
    "\n",
    "JAX依赖[XLA](https://www.tensorflow.org/xla)编译代码，然后在加速卡上执行，比如GPU和TPU。默认情况下，用户不需要关注编译过程，后台自动进行JIT编译并执行，但是JAX支持用户调用JIT(just-in-time)编译自己的Python函数得到XLA优化过的kernel。编译和自动微分可以任意组合，使得你在Python环境下就可以实现性能卓越的复杂算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SY8mDvEvCGqk"
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQ89jHCYfhpg"
   },
   "source": [
    "## 矩阵乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xpy1dSgNqCP4"
   },
   "source": [
    "下面👇🏻的例子中，我们会生成一些随机数。你将会看到，在如何生成随机数这个问题上，Numpy和JAX的处理方式很不同，想了解更多细节可以查看 [Common Gotchas in JAX].\n",
    "\n",
    "[Common Gotchas in JAX]: https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#%F0%9F%94%AA-Random-Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "u0nseKZNqOoH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3721097   0.2642309  -0.18252775 -0.7368085  -0.44030353 -0.15214416\n",
      " -0.6713451  -0.590867    0.7316775   0.567302  ]\n"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(0)\n",
    "x = random.normal(key, (10,))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDJF0UPKnuqB"
   },
   "source": [
    "看一下两个矩阵相乘的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eXn8GUl6CG5N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.08 ms ± 95.3 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "size = 3000\n",
    "x = random.normal(key, (size, size), dtype=jnp.float32)\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()  # runs on the TPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AlN7EbonyaR"
   },
   "source": [
    "我们添加了 `block_until_ready` ，是因为JAX默认使用异步执行。\n",
    "\n",
    "在Numpy array上使用JAX中的函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZPl0MuwYrM7t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.5 ms ± 425 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.normal(size=(size, size)).astype(np.float32)\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SrcB2IurUuE"
   },
   "source": [
    "可以看到上面的代码执行比较慢，因为它需要把数据传输到TPU，你可以手动调用`jax.device_put`将NDArray传输到TPU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Jj7M7zyRskF0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.01 ms ± 23.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "from jax import device_put\n",
    "\n",
    "x = np.random.normal(size=(size, size)).astype(np.float32)\n",
    "x = device_put(x)\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jaxlib.xla_extension.DeviceArray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clO9djnen8qi"
   },
   "source": [
    "`jax.device_put`的输出就像NDArray一样，但是它的类型已经成了DeviceArray，只有当你打印它的值或者保存到硬盘等操作时，JAX会将它的值复制到CPU。 `jax.device_put(x)`和`jit(lambda x: x)(x)`功能相同，但是前者速度更快。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghkfKNQttDpg"
   },
   "source": [
    "如果你有加速卡，比如GPU或TPU，这些代码会在加速卡上运行，通常比CPU上快很多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOzp0P_GoJhb"
   },
   "source": [
    "JAX可不仅仅是支持GPU的Numpy那么简单，它还包含了很多有用的函数转换功能，重要的有限三个:\n",
    "\n",
    " - `jax.jit`, 用于加速你的代码\n",
    " - `jax.grad`, 用于计算微分\n",
    " - `jax.vmap`, 用于自动矢量化或者批处理（batching）\n",
    "\n",
    "让我们一个一个来介绍，最后再看下如何将他们组合发挥更大的作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTTrTbWvgLUK"
   },
   "source": [
    "## 使用`jax.jit`来加速函数执行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrqE32mvE3b7"
   },
   "source": [
    "JAX自动将代码在GPU或TPU上执行，除非这俩都没有，才在CPU上执行。但是，上面的那些例子，JAX每次只向TPU分配一个计算操作，如果我们有多个计算操作，可以使用`@jit`修饰符借助[XLA](https://www.tensorflow.org/xla)将这些计算操作一起编译。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qLGdCtFKFLOR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.33 ms ± 183 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "x = random.normal(key, (1000000,))\n",
    "%timeit selu(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_V8SruVHrD_"
   },
   "source": [
    "我们使用`@jit`进行加速，当第一次调用`selu`时会进行jit编译然后将编译结果缓存供后续调用使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fh4w_3NpFYTp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274 µs ± 3.68 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "selu_jit = jit(selu)\n",
    "%timeit selu_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxpBc4WmfsEU"
   },
   "source": [
    "## 使用`jax.grad`计算微分\n",
    "\n",
    "除了使用数值型函数进行求值，我们还可以对它们进行转换（transform），其中一个转换就是[自动微分（automatic differentiation）](https://en.wikipedia.org/wiki/Automatic_differentiation)。和[Autograd](https://github.com/HIPS/autograd)一样，在JAX中，只需要使用`jax.grad`就可以得到导函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IMAgNJaMJwPD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25       0.1966118  0.10499343]\n"
     ]
    }
   ],
   "source": [
    "def sum_logistic(x):\n",
    "  return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
    "\n",
    "x_small = jnp.arange(3.)\n",
    "derivative_fn = grad(sum_logistic)  # 导函数\n",
    "print(derivative_fn(x_small))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtNs881Ohioc"
   },
   "source": [
    "让我们验证下计算结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JXI7_OZuKZVO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24974345 0.1965761  0.10490417]\n"
     ]
    }
   ],
   "source": [
    "def first_finite_differences(f, x):\n",
    "  eps = 1e-3\n",
    "  return jnp.array([(f(x + eps * v) - f(x - eps * v)) / (2 * eps)\n",
    "                   for v in jnp.eye(len(x))])\n",
    "\n",
    "\n",
    "print(first_finite_differences(sum_logistic, x_small))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2CUZjOWNZ-3"
   },
   "source": [
    "`jax.grad` 和`jax.jit` 可以任意组合，比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TO4g8ny-OEi4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.03532532\n"
     ]
    }
   ],
   "source": [
    "print(grad(jit(grad(jit(grad(sum_logistic)))))(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCJ5feKvhnBJ"
   },
   "source": [
    "对于一些复杂自动微分操作，你还可以使用`jax.vjp`进行 reverse-mode vector-Jacobian products，`jax.jvp` for forward-mode Jacobian-vector products。 这两个函数也可以任意组合，以及搭配其他JAX转换，比如我们看一个计算Hessian矩阵的例子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Z-JxbiNyhxEW"
   },
   "outputs": [],
   "source": [
    "from jax import jacfwd, jacrev\n",
    "def hessian(fun):\n",
    "  return jit(jacfwd(jacrev(fun)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TI4nPsGafxbL"
   },
   "source": [
    "## 使用`jax.vmap`自动矢量化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcxkONy5aius"
   },
   "source": [
    "JAX另一个很有用的转换是`jax.vmap`，也就是矢量化的map。 比起for循环+map，它的速度更快，当搭配`jax.jit`效果更佳。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPiX4y-bWLFS"
   },
   "source": [
    "我们来看一个简单的例子，用`jax.vmap`将矩阵-向量乘法扩展为矩阵-矩阵乘法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8w0Gpsn8WYYj"
   },
   "outputs": [],
   "source": [
    "mat = random.normal(key, (150, 100))\n",
    "batched_x = random.normal(key, (10, 100))\n",
    "\n",
    "def apply_matrix(v):\n",
    "  return jnp.dot(mat, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zWsc0RisQWx"
   },
   "source": [
    "`apply_matrix`是一个矩阵-向量乘法函数，为了实现矩阵-矩阵乘法，我们可以在batch维度进行for循环，但是这样做效率会很差： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "KWVc9BsZv0Ki"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naively batched\n",
      "2.87 ms ± 317 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "def naively_batched_apply_matrix(v_batched):\n",
    "  return jnp.stack([apply_matrix(v) for v in v_batched])\n",
    "\n",
    "print('Naively batched')\n",
    "%timeit naively_batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHfKaLE9stbA"
   },
   "source": [
    "我们知道`jnp.dot`本身支持矩阵-矩阵乘法，所以我们可以重新写一个矩阵乘法函数`batched_apply_matrix`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipei6l8nvrzH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually batched\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def batched_apply_matrix(v_batched):\n",
    "  return jnp.dot(v_batched, mat.T)\n",
    "\n",
    "print('Manually batched')\n",
    "%timeit batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eF8Nhb-szAb"
   },
   "source": [
    "上面的例子很简单，我们重新实现一个函数没问题，但是如果`apply_matrix`本身非常复杂呢，再写一个`batched_apply_matrix`代价就会非常高，这时候就可以借助`jax.vmap`自动让`apply_matrix`支持批处理（batch）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67Oeknf5vuCl"
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def vmap_batched_apply_matrix(v_batched):\n",
    "  return vmap(apply_matrix)(v_batched)\n",
    "\n",
    "print('Auto-vectorized with vmap')\n",
    "%timeit vmap_batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYVl3Z2nbZhO"
   },
   "source": [
    "同样地，`jax.vmap`可以和其他JAX转换任意组合搭配。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "JAX Quickstart.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
